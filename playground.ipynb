{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from google.cloud import storage, bigquery\n",
    "import os\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "AIRFLOW_HOME = '/projects/airflow_finding_the_fastest_way_around_NYC/airflow'\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = f'{AIRFLOW_HOME}/dags/credentials.json'\n",
    "\n",
    "\n",
    "# Access bucket\n",
    "client = storage.Client()\n",
    "bucket = client.get_bucket('citi_bikes_airflow_project')\n",
    "blobs = bucket.list_blobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blobs.num_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generalizing the DAG\n",
    "\n",
    "So far, for the purposes of building the DAG, we have hard-coded the date to download. This is not ideal, as we want the DAG to be able to run for any date.\n",
    "\n",
    "The final step in this article is to generlize the DAG so it would apply to other dates.\n",
    "\n",
    "We will create a function called grab_latest_date() that will return the latest date in the Google Bucket. This function will be called in the DAG to determine the date to download.\n",
    "\n",
    "The function will be called in the DAG as follows:\n",
    "\n",
    "# Grab date to download\n",
    "def grab_latest_date():\n",
    "\n",
    "    # Access bucket\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket('citi_bikes_airflow_project')\n",
    "    blobs = bucket.list_blobs()\n",
    "    latest_date = datetime(2022,1,1)\n",
    "\n",
    "    # Return 202201 for the first run of the DAG\n",
    "    if blobs.num_results == 0:\n",
    "        return latest_date.strftime('%Y%m')\n",
    "\n",
    "    # Loop through blobs to find the latest date\n",
    "    for blob in blobs:\n",
    "        if blob.name.endswith('.zip'):\n",
    "            date = blob.name.split('-')[0]\n",
    "            date = datetime.strptime(date, '%Y%m')\n",
    "            if date > latest_date:\n",
    "                latest_date = date\n",
    "    \n",
    "    # Adding one month to the latest date to get the next month\n",
    "    latest_date = latest_date.strftime('%Y%m')\n",
    "    latest_date = datetime.strptime(latest_date, '%Y%m')\n",
    "    latest_date = latest_date + relativedelta(months=1)\n",
    "    return latest_date.strftime('%Y%m')\n",
    "\n",
    "date_to_download = grab_latest_date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project name: airflow_finding_the_fastest_way_around_NYC\n",
    "\n",
    "introduction\n",
    "\n",
    "This series of articles will walk you through the process of building a data pipeline using Apache Airflow. The data pipeline will be used to download, process, and analyze Citi Bike data from NYC. \n",
    "\n",
    "Citi Bike is a bicycle rental service available on the streets of New York. Users can pick up a bike at one station and ride it to their destination station. The data pipeline will be used to analyze the fastest way to get around NYC.\n",
    "\n",
    "The data pipeline will be built in two articles. \n",
    "\n",
    "The first article will focus on building the DAG to periodically download the data, store it in a Google Bucket as backup, and load it into BigQuery. \n",
    "The second article will focus on building the data processing functions and the DAG to process the data and analyze the fastest way to get around NYC.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airflow_finding_the_fastest_way_around_NYC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
